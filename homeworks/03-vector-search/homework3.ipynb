{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "user_question_embeding=model.encode(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_question_embeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "document_ml_zoomcamp = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ml_zoomcamp=[doc for doc in document_ml_zoomcamp if doc[\"course\"]==\"machine-learning-zoomcamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_ml_zoomcamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'How do I sign up?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'id': '0227b872'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_ml_zoomcamp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0458ca10c6af483e91c4e1a6492f5d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I sign up? Machine Learning Zoomcamp FAQ\n",
      "The purpose of this document is to capture frequently asked technical questions.\n",
      "We did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\n",
      "Data Engineering Zoomcamp FAQ\n",
      "In the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\n",
      "work\n",
      "Is it going to be live? When? The course videos are pre-recorded, you can start watching the course right now.\n",
      "We will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\n",
      "You can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.\n",
      "What if I miss a session? Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.\n",
      "How much theory will you cover? The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\n",
      "For example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\n",
      "I don't know math. Can I take the course? Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\n",
      "Here are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\n",
      "(Mélanie Fouesnard)\n",
      "I filled the form, but haven't received a confirmation email. Is it normal? The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\n",
      "If you unsubscribed from our newsletter, you won't get course related updates too.\n",
      "But don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\n",
      "How long is the course? Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)\n",
      "How much time do I need for this course? Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article\n",
      "Will I get a certificate? Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.\n",
      "Will I get a certificate if I missed the midterm project? Yes, it's possible. See the previous answer.\n",
      "How much Python should I know? Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\n",
      "Introduction to Python – Machine Learning Bookcamp\n",
      "You can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\n",
      "(Mélanie Fouesnard)\n",
      "Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ. For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\n",
      "(Rileen Sinha; based on response by Alexey on Slack)\n",
      "How to setup TensorFlow with GPU support on Ubuntu? Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/\n",
      "I’m new to Slack and can’t find the course channel. Where is it? Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\n",
      "Click “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\n",
      "Browse the list of public channels in your workspace, or use the search bar to search by channel name or description.\n",
      "Select a channel from the list to view it.\n",
      "Click Join Channel.\n",
      "Do we need to provide the GitHub link to only our code corresponding to the homework questions?\n",
      "Yes. You are required to provide the URL to your repo in order to receive a grade\n",
      "The course has already started. Can I still join it? Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\n",
      "In order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.\n",
      "When does the next iteration start? The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).\n",
      "Can I submit the homework after the due date? No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.\n",
      "I just joined. What should I do next? How can I access course materials? Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\n",
      "Click on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\n",
      "Or you can just use this link: http://mlzoomcamp.com/#syllabus\n",
      "What are the deadlines in this course? For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)\n",
      "What’s the difference between the previous iteration of the course (2022) and this one (2023)? There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.\n",
      "The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021? We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\n",
      "If you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.\n",
      "Submitting learning in public links When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\n",
      "For posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\n",
      "The same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\n",
      "For midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.\n",
      "Adding community notes You can create your own github repository for the course with your notes, homework, projects, etc.\n",
      "Then fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\n",
      "After that's done, create a pull request to sync your fork with the original course repo.\n",
      "(By Wesley Barreto)\n",
      "Computing the hash for the leaderboard and project review Leaderboard Links:\n",
      "2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\n",
      "2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\n",
      "Python Code:\n",
      "from hashlib import sha1\n",
      "def compute_hash(email):\n",
      "return sha1(email.lower().encode('utf-8')).hexdigest()\n",
      "You need to call the function as follows:\n",
      "print(compute_hash('YOUR_EMAIL_HERE'))\n",
      "The quotes are required to denote that your email is a string.\n",
      "(By Wesley Barreto)\n",
      "You can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\n",
      "(Mélanie Fouesnard)\n",
      "wget is not recognized as an internal or external command If you get “wget is not recognized as an internal or external command”, you need to install it.\n",
      "On Ubuntu, run\n",
      "sudo apt-get install wget\n",
      "On Windows, the easiest way to install wget is to use Chocolatey:\n",
      "choco install wget\n",
      "Or you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\n",
      "On Mac, the easiest way to install wget is to use brew.\n",
      "Brew install wget\n",
      "Alternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\n",
      "python -m wget\n",
      "You need to install it with pip first:\n",
      "pip install wget\n",
      "And then in your python code, for example in your jupyter notebook, use:\n",
      "import wget\n",
      "wget.download(\"URL\")\n",
      "This should download whatever is at the URL in the same directory as your code.\n",
      "(Memoona Tahira)\n",
      "Alternatively, you can read a CSV file from a URL directly with pandas:\n",
      "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n",
      "df = pd.read_csv(url)\n",
      "Valid URL schemes include http, ftp, s3, gs, and file.\n",
      "In some cases you might need to bypass https checks:\n",
      "import ssl\n",
      "ssl._create_default_https_context = ssl._create_unverified_context\n",
      "Or you can use the built-in Python functionality for downloading the files:\n",
      "import urllib.request\n",
      "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n",
      "urllib.request.urlretrieve(url, \"housing.csv\")\n",
      "Urllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\n",
      "The urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\n",
      "On any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\n",
      "(Mohammad Emad Sharifi)\n",
      "Retrieving csv inside notebook You can use\n",
      "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "To download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\n",
      "For instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\n",
      "!mkdir -p ../data/\n",
      "!mv housing.csv ../data/\n",
      "Windows WSL and VS Code\n",
      "If you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine. (Tyler Simpson)\n",
      "Uploading the homework to Github This is my first time using Github to upload a code. I was getting the below error message when I type\n",
      "git push -u origin master:\n",
      "error: src refspec master does not match any\n",
      "error: failed to push some refs to 'https://github.com/XXXXXX/1st-Homework.git'\n",
      "Solution:\n",
      "The error message got fixed by running below commands:\n",
      "git commit -m \"initial commit\"\n",
      "git push origin main\n",
      "If this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\n",
      "(Asia Saeed)\n",
      "You can also use the “upload file” functionality from GitHub for that\n",
      "If you write your code on Google colab you can also directly share it on your Github.\n",
      "(By Pranab Sarma)\n",
      "Singular Matrix Error I'm trying to invert the matrix but I got error that the matrix is singular matrix\n",
      "The singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\n",
      "Conda is not an internal command I have a problem with my terminal. Command\n",
      "conda create -n ml-zoomcamp python=3.9\n",
      "doesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\n",
      "If you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\n",
      "If you don’t have Anaconda or Miniconda, you should install it first\n",
      "(Tatyana Mardvilko)\n",
      "Read-in the File in Windows OS How do I read the dataset with Pandas in Windows?\n",
      "I used the code below but not working\n",
      "df = pd.read_csv('C:\\Users\\username\\Downloads\\data.csv')\n",
      "Unlike Linux/Mac OS, Windows uses the backslash (\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the '\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\n\" to add a new line or \"\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\n",
      "Here’s how we should be loading the file instead:\n",
      "df = pd.read_csv(r'C:\\Users\\username\\Downloads\\data.csv')\n",
      "(Muhammad Awon)\n",
      "'403 Forbidden' error message when you try to push to a GitHub repository Type the following command:\n",
      "git config -l | grep url\n",
      "The output should look like this:\n",
      "remote.origin.url=https://github.com/github-username/github-repository-name.git\n",
      "Change this to the following format and make sure the change is reflected using command in step 1:\n",
      "git remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\n",
      "(Added by Dheeraj Karra)\n",
      "Fatal: Authentication failed for 'https://github.com/username I had a problem when I tried to push my code from Git Bash:\n",
      "remote: Support for password authentication was removed on August 13, 2021.\n",
      "remote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\n",
      "fatal: Authentication failed for 'https://github.com/username\n",
      "Solution:\n",
      "Create a personal access token from your github account and use it when you make a push of your last changes.\n",
      "https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n",
      "Bruno Bedón\n",
      "wget: unable to resolve host address 'raw.githubusercontent.com' In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\n",
      "Getting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address 'raw.githubusercontent.com'\n",
      "Solution:\n",
      "In your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\n",
      "Setting up an environment using VS Code I found this video quite helpful: Creating Virtual Environment for Python from VS Code\n",
      "[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\n",
      "[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\n",
      "(Added by Ivan Brigida)\n",
      "Conda Environment Setup With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\n",
      "Answer:\n",
      "\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\n",
      "(Added by Wesley Barreto)\n",
      "conda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml\n",
      "Floating Point Precision I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\n",
      "Inverse * Original:\n",
      "[[ 1.00000000e+00 -1.38777878e-16]\n",
      "[ 3.16968674e-13  1.00000000e+00]]\n",
      "Solution:\n",
      "It's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\n",
      "(Added by Wesley Barreto)\n",
      "What does pandas.DataFrame.info() do? Answer:\n",
      "It prints the information about the dataset like:\n",
      "Index datatype\n",
      "No. of entries\n",
      "Column information with not-null count and datatype\n",
      "Memory usage by dataset\n",
      "We use it as:\n",
      "df.info()\n",
      "(Added by Aadarsha Shrestha & Emoghena Itakpe)\n",
      "NameError: name 'np' is not defined Pandas and numpy libraries are not being imported\n",
      "NameError: name 'np' is not defined\n",
      "NameError: name 'pd' is not defined\n",
      "If you're using numpy or pandas, make sure you use the first few lines before anything else.\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "Added by Manuel Alejandro Aponte\n",
      "How to select column by dtype What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\n",
      "df.select_dtypes(include=np.number).columns.tolist()\n",
      "df.select_dtypes(include='object').columns.tolist()\n",
      "Added by Gregory Morris\n",
      "How to identify the shape of dataset in Pandas There are many ways to identify the shape of dataset, one of them is using .shape attribute!\n",
      "df.shape\n",
      "df.shape[0] # for identify the number of rows\n",
      "df.shape[1] # for identify the number of columns\n",
      "Added by Radikal Lukafiardi\n",
      "How to avoid Value errors with array shapes in homework? First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\n",
      "Dimension Mismatch\n",
      "To perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\n",
      "Added by Leah Gotladera\n",
      "Question 5: How and why do we replace the NaN values with average of the column? You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\n",
      "This method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\n",
      "Added by Anneysha Sarkar\n",
      "Question 7: Mathematical formula for linear regression In Question 7 we are asked to calculate\n",
      "The initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\n",
      "Additional reading and videos:\n",
      "Ordinary least squares\n",
      "Multiple Linear Regression in Matrix Form\n",
      "Pseudoinverse Solution to OLS\n",
      "Added by Sylvia Schmitt\n",
      "with commends from Dmytro Durach\n",
      "Question 7: FINAL MULTIPLICATION not having 5 column This is most likely that you interchanged the first step of the multiplication\n",
      "You used  instead of\n",
      "Added by Emmanuel Ikpesu\n",
      "Question 7: Multiplication operators. Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\n",
      "numpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\n",
      "If multiplying by a scalar numpy.multiply() or * is preferred.\n",
      "Added by Andrii Larkin\n",
      "Error launching Jupyter notebook If you face an error kind of ImportError: cannot import name 'contextfilter' from 'jinja2' (anaconda\\lib\\site-packages\\jinja2\\__init__.py) when launching a new notebook for a brand new environment.\n",
      "Switch to the main environment and run \"pip install nbconvert --upgrade\".\n",
      "Added by George Chizhmak\n",
      "wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1 If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again\n",
      "In case you are using mac os and having trouble with WGET Wget doesn't ship with macOS, so there are other alternatives to use.\n",
      "No worries, we got curl:\n",
      "example:\n",
      "curl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Explanations:\n",
      "curl: a utility for retrieving information from the internet.\n",
      "-o: Tell it to store the result as a file.\n",
      "filename: You choose the file's name.\n",
      "Links: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\n",
      "More about it at:\n",
      "Curl Documentation\n",
      "Added by David Espejo\n",
      "How to output only a certain number of decimal places You can use round() function or f-strings\n",
      "round(number, 4)  - this will round number up to 4 decimal places\n",
      "print(f'Average mark for the Homework is {avg:.3f}') - using F string\n",
      "Also there is pandas.Series. round idf you need to round values in the whole Series\n",
      "Please check the documentation\n",
      "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\n",
      "Added by Olga Rudakova\n",
      "How do I get started with Week 2? Here are the crucial links for this Week 2 that starts September 18, 2023\n",
      "Ask questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\n",
      "Calendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\n",
      "Week 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\n",
      "Submit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "Youtube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\n",
      "FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n",
      "~~Nukta Bhatia~~\n",
      "Checking long tail of data We can use histogram:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "# Load the data\n",
      "url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\n",
      "df = pd.read_csv(url)\n",
      "# EDA\n",
      "sns.histplot(df['median_house_value'], kde=False)\n",
      "plt.show()\n",
      "OR ceck skewness and describe:\n",
      "print(df['median_house_value'].describe())\n",
      "# Calculate the skewness of the 'median_house_value' variable\n",
      "skewness = df['median_house_value'].skew()\n",
      "# Print the skewness value\n",
      "print(\"Skewness of 'median_house_value':\", skewness)\n",
      "(Mohammad Emad Sharifi)\n",
      "LinAlgError: Singular matrix It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\n",
      "You can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\n",
      "(Added by Cécile Guillot)\n",
      "California housing dataset You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\n",
      "KS\n",
      "Getting NaNs after applying .mean() I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\n",
      "I found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\n",
      "Added by Sasmito Yudha Husada\n",
      "Target variable transformation Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\n",
      "Only if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\n",
      "This can help to understand skewness and how it can be applied to the distribution of your data set.\n",
      "https://en.wikipedia.org/wiki/Skewness\n",
      "Pastor Soto\n",
      "Reading the dataset directly from github The dataset can be read directly to pandas dataframe from the github link using the technique shown below\n",
      "dfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\n",
      "Krishna Anand\n",
      "Loading the dataset directly through Kaggle Notebooks For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\n",
      "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Once the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\n",
      "df = pd.read_csv('housing.csv')\n",
      "Harish Balasundaram\n",
      "Filter a dataset by using its values We can filter a dataset by using its values as below.\n",
      "df = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\n",
      "You can use | for ‘OR’, and & for ‘AND’\n",
      "Alternative:\n",
      "df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
      "Radikal Lukafiardi\n",
      "Alternative way to load the data using requests Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\n",
      "# Get data for homework\n",
      "import requests\n",
      "url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\n",
      "response = requests.get(url)\n",
      "if response.status_code == 200:\n",
      "with open('housing.csv', 'wb') as file:\n",
      "file.write(response.content)\n",
      "else:\n",
      "print(\"Download failed.\")\n",
      "Tyler Simpson\n",
      "Null column is appearing even if I applied .fillna() When creating a duplicate of your dataframe by doing the following:\n",
      "X_train = df_train\n",
      "X_val = df_val\n",
      "You’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\n",
      "X_train = df_train.copy()\n",
      "X_val = df_val.copy()\n",
      "Added by Ixchel García\n",
      "Can I use Scikit-Learn’s train_test_split for this week? Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it\n",
      "Can I use LinearRegression from Scikit-Learn for this week? Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.\n",
      "Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization) What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\n",
      "Corresponding function for model without regularization:\n",
      "sklearn.linear_model.LinearRegression\n",
      "Corresponding function for model with regularization:\n",
      "sklearn.linear_model.Ridge\n",
      "The linear model from Scikit-Learn are explained  here:\n",
      "https://scikit-learn.org/stable/modules/linear_model.html\n",
      "Added by Sylvia Schmitt\n",
      "Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()? `r` is a regularization parameter.\n",
      "It’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here's how both are used:\n",
      "sklearn.Ridge()\n",
      "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      "lesson’s notebook (`train_linear_regression_reg` function)\n",
      "XTX = XTX + r * np.eye(XTX.shape[0])\n",
      "`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.\n",
      "Why linear regression doesn’t provide a “perfect” fit? Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\n",
      "A: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\n",
      "As our model is linear, how would you draw a line to fit all the \"dots\"?\n",
      "You could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\n",
      "Added by Andrii Larkin\n",
      "Random seed 42 One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\n",
      "The purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.\n",
      "Shuffling the initial dataset using pandas built-in function It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\n",
      "Setting frac=1 will result in returning a shuffled version of the complete Dataset.\n",
      "Setting random_state=seed will result in the same randomization as used in the course resources.\n",
      "df_shuffled = df.sample(frac=1, random_state=seed)\n",
      "df_shuffled.reset_index(drop=True, inplace=True)\n",
      "Added by Sylvia Schmitt\n",
      "The answer I get for one of the homework questions doesn't match any of the options. What should I do? That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\n",
      "If it’s the case, just select the option that’s closest to your answer\n",
      "Meaning of mean in homework 2, question 3 In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\n",
      "It means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\n",
      "df_train['column_name'].mean( )\n",
      "Another option:\n",
      "df_train[‘column_name’].describe()\n",
      "(Bhaskar Sarma)\n",
      "When should we transform the target variable to logarithm distribution? When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work\n",
      "ValueError: shapes not aligned If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\n",
      "If this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\n",
      "(Santhosh Kumar)\n",
      "How to copy a dataframe without changing the original dataframe? Copy of a dataframe is made with X_copy = X.copy().\n",
      "This is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\n",
      "Any changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\n",
      "(Memoona Tahira)\n",
      "What does ‘long tail’ mean? One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\n",
      "(Tatiana Dávila)\n",
      "What is standard deviation? In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\n",
      "(Aadarsha Shrestha)\n",
      "Do we need to apply regularization techniques always? Or only in certain scenarios? The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\n",
      "(Daniel Muñoz Viveros)\n",
      "Shortcut: define functions for faster execution As it speeds up the development:\n",
      "prepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\n",
      "Of course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\n",
      "(Ivan Brigida)\n",
      "How to use pandas to find standard deviation If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\n",
      "(Quinn Avila)\n",
      "Standard Deviation Differences in Numpy and Pandas Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\n",
      "Numpy\n",
      "Pandas\n",
      "pandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\n",
      "import numpy as np\n",
      "np.std(df.weight, ddof=1)\n",
      "The result will be similar if we change the dof = 1 in numpy\n",
      "(Harish Balasundaram)\n",
      "Standard deviation using Pandas built in Function In pandas you can use built in Pandas function names std() to get standard deviation. For example\n",
      "df['column_name'].std() to get standard deviation of that column.\n",
      "df[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\n",
      "(Khurram Majeed)\n",
      "How to combine train and validation datasets Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\n",
      "df_train_combined = pd.concat([df_train, df_val])\n",
      "y_train = np.concatenate((y_train, y_val), axis=0)\n",
      "(George Chizhmak)\n",
      "Understanding RMSE and how to calculate RMSE score The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model's predicted values and the actual observed values, offering insight into the model's ability to accurately forecast the target variable. To calculate RMSE score:\n",
      "Libraries needed\n",
      "import numpy as np\n",
      "from sklearn.metrics import mean_squared_error\n",
      "mse = mean_squared_error(actual_values, predicted_values)\n",
      "rmse = np.sqrt(mse)\n",
      "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
      "(Aminat Abolade)\n",
      "What syntax use in Pandas for multiple conditions using logical AND and OR If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\n",
      "(Olga Rudakova)\n",
      "–\n",
      "Deep dive into normal equation for regression I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression\n",
      "Useful Resource for Missing Data Treatment\n",
      "https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook (Hrithik Kumar Advani)\n",
      "Caution for applying log transformation in Week-2 2023 cohort homework The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\n",
      "However, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\n",
      "(Added by Soham Mundhada)\n",
      "What sklearn version is Alexey using in the youtube videos? Version 0.24.2 and Python 3.8.11\n",
      "(Added by Diego Giraldo)\n",
      "How do I get started with Week 3? Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\n",
      "Submit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "Youtube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\n",
      "~~Nukta Bhatia~~\n",
      "Could not convert string to float:’Nissan’rt string to float: 'Nissan' The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\n",
      "To resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\n",
      "Here’s an example of how you can perform one-hot encoding using pandas:\n",
      "import pandas as pd\n",
      "# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\n",
      "data_encoded = pd.get_dummies(data, columns=['brand'])\n",
      "In this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\n",
      "-Mohammad Emad Sharifi-\n",
      "Why did we change the targets to binary format when calculating mutual information score in the homework? Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\n",
      "—Odimegwu David—-\n",
      "What data should we use for correlation matrix Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\n",
      "Yes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\n",
      "Pastor Soto\n",
      "Coloring the background of the pandas.DataFrame.corr correlation matrix directly The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\n",
      "Here an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\n",
      "# ensure to have only numerical values in the dataframe before calling 'corr'\n",
      "corr_mat = df_numerical_only.corr()\n",
      "corr_mat.style.background_gradient(cmap='viridis')\n",
      "Here is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\n",
      "np.random.seed = 3\n",
      "df_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\n",
      "df_random.style.background_gradient(cmap='viridis')\n",
      "Added by Sylvia Schmitt\n",
      "Identifying highly correlated feature pairs easily through unstack data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\n",
      "data_corr.head(10)\n",
      "Added by Harish Balasundaram\n",
      "You can also use seaborn to create a heatmap with the correlation. The code for doing that:\n",
      "sns.heatmap(df[numerical_features].corr(),\n",
      "annot=True,\n",
      "square=True,\n",
      "fmt=\".2g\",\n",
      "cmap=\"crest\")\n",
      "Added by Cecile Guillot\n",
      "You can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\n",
      "Which outputs, in the case of churn dataset:\n",
      "(Mélanie Fouesnard)\n",
      "What data should be used for EDA? Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\n",
      "It's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\n",
      "Alena Kniazeva\n",
      "Fitting DictVectorizer on validation Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\n",
      "Edidiong Esu\n",
      "Below is an extract of Alexey's book explaining this point. Hope is useful\n",
      "When we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\n",
      "With this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven't seen. By not applying the fit to the validation model we can know how well it was trained.\n",
      "Below is an extract of Alexey's book explaining this point.\n",
      "Humberto Rodriguez\n",
      "There is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\n",
      "The correct way is to fit_transform the train set, and only transform the validation and test sets.\n",
      "Memoona Tahira\n",
      "Feature elimination For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\n",
      "We should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\n",
      "If the difference is negative, it means that the model actually became better when we removed the feature.\n",
      "FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2 Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\n",
      "Santhosh Kumar\n",
      "Logistic regression crashing Jupyter kernel Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\n",
      "Make sure that the target variable for the logistic regression is binary.\n",
      "Konrad Muehlberg\n",
      "Understanding Ridge Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\n",
      "sag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It's particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\n",
      "Alpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\n",
      "from sklearn.linear_model import Ridge\n",
      "ridge = Ridge(alpha=alpha, solver='sag', random_state=42)\n",
      "ridge.fit(X_train, y_train)\n",
      "Aminat Abolade\n",
      "pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings: DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\n",
      "Using “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\n",
      "Larkin Andrii\n",
      "Convergence Problems in W3Q6 Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "Play with different scalers. See notebook-scaling-ohe.ipynb\n",
      "Dmytro Durach\n",
      "(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.\n",
      "Dealing with Convergence in Week 3 q6 When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\n",
      "Feature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \tsimilar scale, preventing convergence issues.\n",
      "Categorical Feature Encoding: If your dataset includes categorical features, apply \tcategorical encoding techniques such as OneHotEncoder (OHE) to \tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\n",
      "Combine Features: After \tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\n",
      "By following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\n",
      "You can find an example here.\n",
      " \t\t\t\t\t\t\t\t\t\t\t\tOsman Ali\n",
      "Sparse matrix compared dense matrix A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\n",
      "The default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\n",
      " \t\t\t\t\t\t\t\t\t\t\t\tQuinn Avila\n",
      "How  to Disable/avoid Warnings in Jupyter Notebooks The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\n",
      "Import warnings\n",
      "warnings.filterwarnings(“ignore”)\n",
      "Krishna Anand\n",
      "How to select the alpha parameter in Q6 Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\n",
      "Answer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\n",
      "Asia Saeed\n",
      "Second variable that we need to use to calculate the mutual information score Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\n",
      "Answer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\n",
      "Asia Saeed\n",
      "Features for homework Q5 Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\n",
      "You need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\n",
      "While calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\n",
      "Since order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)\n",
      "What is the difference between OneHotEncoder and DictVectorizer? Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\n",
      "Both will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\n",
      "Tanya Mard\n",
      "What is the difference between pandas get_dummies and sklearn OnehotEncoder? They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]\n",
      "Use of random seed in HW3 For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\n",
      "Answer: for both splits random_state = 42 should be used\n",
      "(Bhaskar Sarma)\n",
      "Correlation before or after splitting the data Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\n",
      "Answer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.\n",
      "Features in Ridge Regression Model Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\n",
      "Drop all categorical features first before proceeding.\n",
      "(Aileah Gotladera)\n",
      "While it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\n",
      "(Erjon)\n",
      "Handling Column Information for Homework 3 Question 6 You need to use all features. and price for target. Don't include the average variable we created before.\n",
      "If you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\n",
      "I also used StandardScalar for numerical variable you can try running with or without this\n",
      "(Peter Pan)\n",
      "Transforming Non-Numerical Columns into Numerical Columns Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.\n",
      "What is the better option FeatureHasher or DictVectorizer These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\n",
      "When you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\n",
      "You can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\n",
      "Olga Rudakova\n",
      "Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way? (Question by Connie S.)\n",
      "The reason it's good/recommended practice to do it after splitting is to avoid data leakage - you don't want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\n",
      "Answered/added by Rileen Sinha\n",
      "HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option? If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\n",
      "Added by Akshar Goyal\n",
      "How to calculate Root Mean Squared Error? We can use sklearn & numpy packages to calculate Root Mean Squared Error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "Rmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\n",
      "Added by Radikal Lukafiardi\n",
      "You can also refer to Alexey’s notebook for Week 2:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\n",
      "which includes the following code:\n",
      "def rmse(y, y_pred):\n",
      "error = y_pred - y\n",
      "mse = (error ** 2).mean()\n",
      "return np.sqrt(mse)\n",
      "(added by Rileen Sinha)\n",
      "AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names' The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
      "George Chizhmak\n",
      "Root Mean Squared Error To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
      "See details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\n",
      "Ahmed Okka\n",
      "Encoding Techniques This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\n",
      "Hrithik Kumar Advani\n",
      "Error in use of accuracy_score from sklearn in jupyter (sometimes) I got this error multiple times here is the code:\n",
      "“accuracy_score(y_val, y_pred >= 0.5)”\n",
      "TypeError: 'numpy.float64' object is not callable\n",
      "I solve it using\n",
      "from sklearn import metrics\n",
      "metrics.accuracy_score(y_train, y_pred>= 0.5)\n",
      "OMAR Wael\n",
      "How do I get started with Week 4? Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\n",
      "Sci-Kit Learn on Evaluation:\n",
      "https://scikit-learn.org/stable/model_selection.html\n",
      "~~Nukta Bhatia~~\n",
      "Using a variable to score https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\n",
      "Metrics can be used on a series or a dataframe\n",
      "~~Ella Sahnan~~\n",
      "Why do we sometimes use random_state and not at other times? Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\n",
      "Refer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\n",
      "~~Ella Sahnan~~\n",
      "How to get all classification metrics? How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\n",
      "Use classification_report from sklearn. For more info check here.\n",
      "Abhishek N\n",
      "Multiple thresholds for Q4 I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\n",
      "Choose the one closest to any of the options\n",
      "Added by Azeez Enitan Edunwale\n",
      "You can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\n",
      "Added by Rileen Sinha\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0 Solution description: duplicating the\n",
      "df.churn = (df.churn == 'yes').astype(int)\n",
      "This is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\n",
      "It is telling us that it only contains 0's.\n",
      "Delete one of the below cells and you will get the accuracy\n",
      "Humberto Rodriguez\n",
      "Method to get beautiful classification report Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\n",
      "Krishna Annad\n",
      "I’m not getting the exact result in homework That’s fine, use the closest option\n",
      "Use AUC to evaluate feature importance of numerical variables Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.\n",
      "Help with understanding: “For each numerical value, use it as score and compute AUC” When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\n",
      "Sylvia Schmitt\n",
      "What dataset should I use to compute the metrics in Question 3 You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\n",
      "Diego Giraldo\n",
      "What does KFold do? What does this line do?\n",
      "KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
      "If I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\n",
      "Did you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
      "In my case changing random state changed results\n",
      "(Arthur Minakhmetov)\n",
      "Changing the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\n",
      "(Bhaskar Sarma)\n",
      "In case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\n",
      "(Ani Mkrtumyan)\n",
      "ValueError: multi_class must be in ('ovo', 'ovr') I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\n",
      "I was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\n",
      "Asia Saeed\n",
      "Monitoring Wait times and progress of the code execution can be done with: from tqdm.auto import tqdm\n",
      "Tqdm - terminal progress bar\n",
      "Krishna Anand\n",
      "What is the use of inverting or negating the variables less than the threshold? Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\n",
      "Aileah Gotladera\n",
      "Difference between predict(X) and predict_proba(X)[:, 1] In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\n",
      "The solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\n",
      "Vladimir Yesipov\n",
      "Predict_proba shows probailites per class.\n",
      "Ani Mkrtumyan\n",
      "Why are FPR and TPR equal to 0.0, when threshold = 1.0? For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\n",
      "The threshold is 1.0\n",
      "FPR is 0.0\n",
      "And TPR is 0.0\n",
      "When the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\n",
      "That is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\n",
      "Alena Kniazeva\n",
      "How can I annotate a graph? Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\n",
      "plt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\nOptimal F1 Score: {optimal_f1_score:.2f}',\n",
      "xy=(optimal_threshold, optimal_f1_score),\n",
      "xytext=(0.3, 0.5),\n",
      "textcoords='axes fraction',\n",
      "arrowprops=dict(facecolor='black', shrink=0.05))\n",
      "Quinn Avila\n",
      "I didn’t fully understand the ROC curve. Can I move on? It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\n",
      "Nonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\n",
      "Why do I have different values of accuracy than the options in the homework? One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\n",
      "Although the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\n",
      "1)\n",
      "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
      "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
      "2)\n",
      "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
      "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
      "Therefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\n",
      "Ibraheem Taha\n",
      "How to find the intercept between precision and recall curves by using numpy? You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\n",
      "I suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\n",
      "You want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\n",
      "idx = np.argwhere(\n",
      "np.diff(\n",
      "np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\n",
      ")\n",
      ").flatten()\n",
      "You can print the result to easily read it:\n",
      "print(\n",
      "f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx]['threshold']}.\"\n",
      ")\n",
      "(Mélanie Fouesnard)\n",
      "Compute Recall, Precision, and F1 Score using scikit-learn library In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\n",
      "from sklearn.metrics import precision_score, recall_score, f1_score\n",
      "precision_score(y_true, y_pred, average='binary')\n",
      "recall_score(y_true, y_pred, average='binary')\n",
      "f1_score(y_true, y_pred, average='binary')\n",
      "Radikal Lukafiardi\n",
      "Why do we use cross validation? Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\n",
      "\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\n",
      "Smaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\n",
      "Larger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\n",
      "Aminat Abolade\n",
      "Evaluate the Model using scikit learn metrics Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\n",
      "from sklearn.metrics import (accuracy_score,\n",
      "precision_score,\n",
      "recall_score,\n",
      "f1_score,\n",
      "roc_auc_score\n",
      ")\n",
      "accuracy = accuracy_score(y_val, y_pred)\n",
      "precision = precision_score(y_val, y_pred)\n",
      "recall = recall_score(y_val, y_pred)\n",
      "f1 = f1_score(y_val, y_pred)\n",
      "roc_auc = roc_auc_score(y_val, y_pred)\n",
      "print(f'Accuracy: {accuracy}')\n",
      "print(f'Precision: {precision}')\n",
      "print(f'Recall: {recall}')\n",
      "print(f'F1-Score: {f1}')\n",
      "print(f'ROC AUC: {roc_auc}')\n",
      "(Harish Balasundaram)\n",
      "Are there other ways to compute Precision, Recall and F1 score? Scikit-learn offers another way: precision_recall_fscore_support\n",
      "Example:\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "(Gopakumar Gopinathan)\n",
      "When do I use ROC vs Precision-Recall curves? - ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n",
      "- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\n",
      "-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\n",
      "- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\n",
      "(Anudeep Vanjavakam)\n",
      "How to evaluate feature importance for numerical variables with AUC? You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\n",
      "(Denys Soloviov)\n",
      "Dependence of the F-score on class imbalance Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\n",
      "(George Chizhmak)\n",
      "Quick way to plot Precision-Recall Curve We can import precision_recall_curve from scikit-learn and plot the graph as follows:\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "precision, recall, thresholds = precision_recall_curve(y_val, y_predict)\n",
      "plt.plot(thresholds, precision[:-1], label='Precision')\n",
      "plt.plot(thresholds, recall[:-1], label='Recall')\n",
      "plt.legend()\n",
      "Hrithik Kumar Advani\n",
      "What is Stratified k-fold? For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\n",
      "Please check the realisation in sk-learn library:\n",
      "https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\n",
      "Olga Rudakova\n",
      "How do I get started with Week 5? Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "HW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\n",
      "~~~ Nukta Bhatia ~~~\n",
      "Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc. While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\n",
      "It is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\n",
      "https://www.youtube.com/watch?v=IXSiYkP23zo\n",
      "Note that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\n",
      "Alternative ways are sketched here:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md\n",
      "How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience You’ll need a kaggle account\n",
      "Go to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\n",
      "In the same location as your Jupyter NB, place the `kaggle.json` file\n",
      "Run `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\n",
      "Make sure to import os via `import os` and then run:\n",
      "os.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\n",
      "Finally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\n",
      "And then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\n",
      ">>> Michael Fronda <<<\n",
      "Basic Ubuntu Commands: Cd .. (go back)\n",
      "Ls (see current folders)\n",
      "Cd ‘path’/ (go to this path)\n",
      "Pwd (home)\n",
      "Cat “file name’ --edit txt file in ubuntu\n",
      "Aileah Gotladera\n",
      "Installing and updating to the python version 3.10 and higher Open terminal and type the code below to check the version on your laptop\n",
      "python3 --version\n",
      "For windows,\n",
      "Visit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\n",
      "Run the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\n",
      "Or\n",
      "For Python 3,\n",
      "Open your command prompt or terminal and run the following command:\n",
      "pip install --upgrade python\n",
      "Aminat Abolade\n",
      "How to install WSL on Windows 10 and 11 ? It is quite simple, and you can follow these instructions here:\n",
      "https://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\n",
      "Make sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\n",
      "In the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\n",
      "Once it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\n",
      "You are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\n",
      "To go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\n",
      "Python should be already installed but you can check it by running sudo apt install python3 command.\n",
      "You can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\n",
      "You can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\n",
      "You have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\n",
      "You can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\n",
      "You will need to install pip by running this command sudo apt install python3-pip\n",
      "NB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\n",
      "/sbin/ldconfig.real: Can't link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\n",
      "/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\n",
      "So I had to create the following symbolic link:\n",
      "sudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\n",
      "(Mélanie Fouesnard)\n",
      "Error building Docker images on Mac with M1 silicon Do you get errors building the Docker image on the Mac M1 chipset?\n",
      "The error I was getting was:\n",
      "Could not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\n",
      "The fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "Open mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\n",
      "Replace line 1 with\n",
      "FROM --platform=linux/amd64 ubuntu:latest\n",
      "Now build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\n",
      "David Colton\n",
      "Method to find the version of any install python libraries in jupyter notebook Import waitress\n",
      "print(waitress.__version__)\n",
      "Krishna Anand\n",
      "Cannot connect to the docker daemon. Is the Docker daemon running? Working on getting Docker installed - when I try running hello-world I am getting the error.\n",
      "Docker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\n",
      "Solution description\n",
      "If you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\n",
      "On Linux, start the docker daemon with either of these commands:\n",
      "sudo dockerd\n",
      "sudo service docker start\n",
      "Added by Ugochukwu Onyebuchi\n",
      "The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1 After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\n",
      "In your Dockerfile, change the Python version in the first line the Python version installed in your system:\n",
      "FROM python:3.7.5-slim\n",
      "To find your python version, use the command python --version. For example:\n",
      "python --version\n",
      ">> Python 3.9.7\n",
      "Then, change it on your Dockerfile:\n",
      "FROM python:3.9.7-slim\n",
      "Added by Filipe Melo\n",
      "Running “pipenv install sklearn==1.0.2” gives errors. What should I do? When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\n",
      "The solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\n",
      "Odimegwu David\n",
      "Homework asks you to install 1.3.1\n",
      "Pipenv install scikit-learn==1.3.1\n",
      "Use Pipenv to install Scikit-Learn version 1.3.1\n",
      "Gopakumar Gopinathan\n",
      "Why do we need the --rm flag What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\n",
      "For best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\n",
      "They consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\n",
      "The right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\n",
      "The option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\n",
      "During development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\n",
      "Added by Muhammad Awon\n",
      "Failed to read Dockerfile When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\n",
      "Added by Pastor Soto\n",
      "Install docker on MacOS Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.\n",
      "I cannot pull the image with docker pull command Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\n",
      "Using default tag: latest\n",
      "Error response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\n",
      "Solution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\n",
      "docker pull svizor/zoomcamp-model:3.10.12-slim\n",
      "Added by Vladimir Yesipov\n",
      "Dumping/Retrieving only the size of for a specific Docker image Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\n",
      "docker image ls <image name>\n",
      "Or alternatively:\n",
      "docker images <image name>\n",
      "In action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\n",
      "docker image ls --format \"{{.Size}}\" <image name>\n",
      "Or alternatively:\n",
      "docker images --format \"{{.Size}}\" <image name>\n",
      "Sylvia Schmitt\n",
      "Where does pipenv create environments and how does it name them? It creates them in\n",
      "OSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\n",
      "Windows: C:\\Users\\<USERNAME>\\.virtualenvs\\folder-name_cyrptic-hash\n",
      "Eg: C:\\Users\\Ella\\.virtualenvs\\code-qsdUdabf (for module-05 lesson)\n",
      "The environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\n",
      "All libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\n",
      "(Memoona Tahira)\n",
      "How do I debug a docker container? Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\n",
      "docker run -it --entrypoint bash <image>\n",
      "If the container is already running, execute a command in the specific container:\n",
      "docker ps (find the container-id)\n",
      "docker exec -it <container-id> bash\n",
      "(Marcos MJD)\n",
      "The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash) $ docker exec -it 1e5a1b663052 bash\n",
      "the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\n",
      "Fix:\n",
      "winpty docker exec -it 1e5a1b663052 bash\n",
      "A TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\n",
      "Winpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\n",
      "More info on terminal, shell, console applications hi and so on:\n",
      "https://conemu.github.io/en/TerminalVsShell.html\n",
      "(Marcos MJD)\n",
      "Error: failed to compute cache key: \"/model2.bin\" not found: not found Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\n",
      "COPY [\"model2.bin\", \"dv.bin\", \"./\"]\n",
      "then I got the error above in MINGW64 (git bash) on Windows.\n",
      "The temporary solution I found was to use\n",
      "COPY [\"*\", \"./\"]\n",
      "which I assume combines all the files from the original docker image and the files in your working directory.\n",
      "Added by Muhammed Tan\n",
      "Failed to write the dependencies to pipfile and piplock file Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\n",
      "Krishna Anand\n",
      "f-strings f-String not properly keyed in: does anyone knows why i am getting error after import pickle?\n",
      "The first error showed up because your f-string is using () instead of {} around C. So, should be: f’model_C={C}.bin’\n",
      "The second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)\n",
      "(Humberto R.)\n",
      "'pipenv' is not recognized as an internal or external command, operable program or batch file. This error happens because pipenv is already installed but you can't access it from the path.\n",
      "This error comes out if you run.\n",
      "pipenv  --version\n",
      "pipenv shell\n",
      "Solution for Windows\n",
      "Open this option\n",
      "Click here\n",
      "Click in Edit Button\n",
      "Make sure the next two locations are on the PATH, otherwise, add it.\n",
      "C:\\Users\\AppData\\....\\Python\\PythonXX\\\n",
      "C:\\Users\\AppData\\....\\Python\\PythonXX\\Scripts\\\n",
      "Added by Alejandro Aponte\n",
      "Note: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\n",
      "AttributeError: module ‘collections’ has no attribute ‘MutableMapping’ Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\n",
      "Solution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\n",
      "Added by Hareesh Tummala\n",
      "Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts') After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\n",
      "It can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\n",
      "# for Windows\n",
      "set VIRTUAL_ENV \"\"\n",
      "# for Unix\n",
      "export VIRTUAL_ENV=\"\"\n",
      "Also manually re-creating removed folder at `C:\\Users\\username\\.virtualenvs\\removed-envname` can help, removed-envname can be seen at the error message.\n",
      "Added by Andrii Larkin\n",
      "ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')) Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\n",
      "(Theresa S.)\n",
      "docker  build ERROR [x/y] COPY … Solution:\n",
      "This error occurred because I used single quotes around the filenames. Stick to double quotes\n",
      "Fix error during installation of Pipfile inside Docker container I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\n",
      "RUN pipenv install --system --deploy --ignore-pipfile\n",
      "How to fix error after running the Docker run command Solution\n",
      "This error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\n",
      "Running the following commands\n",
      "docker ps -a <to list all docker containers>\n",
      "docker images <to list images>\n",
      "docker stop <container ID>\n",
      "docker rm <container ID>\n",
      "docker rmi image\n",
      "I rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.\n",
      "Bind for 0.0.0.0:9696 failed: port is already allocated I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\n",
      "Error message:\n",
      "Error response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\n",
      "Solution description\n",
      "Issue has been resolved running the following command:\n",
      "docker kill $(docker ps -q)\n",
      "https://github.com/docker/for-win/issues/2722\n",
      "Asia Saeed\n",
      "Bind for 127.0.0.1:5000 showing error I was getting the error on client side with this\n",
      "Client Side:\n",
      "File \"C:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen …………………..\n",
      "raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Sevrer Side:\n",
      "It showed error for gunicorn\n",
      "The waitress  cmd was running smoothly from server side\n",
      "Solution:\n",
      "Use the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\n",
      "Aamir Wani\n",
      "Installing md5sum on Macos Install it by using command\n",
      "% brew install md5sha1sum\n",
      "Then run command to check hash for file to check if they the same with the provided\n",
      "% md5sum model1.bin dv.bin\n",
      "Olga Rudakova\n",
      "How to run a script while a web-server is working? Problem description:\n",
      "I started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\n",
      "Solution description:\n",
      "Just open another terminal (command window, powershell, etc.) and run a python script.\n",
      "Alena Kniazeva\n",
      "Version-conflict in pipenv Problem description:\n",
      "In video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\n",
      "UserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Solution description:\n",
      "When you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\n",
      "Bhaskar Sarma\n",
      "Python_version and Python_full_version error after running pipenv install: If you install packages via pipenv install, and get an error that ends like this:\n",
      "pipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\n",
      "python_full_version: 'python_version' must not be present with 'python_full_version'\n",
      "python_version: 'python_full_version' must not be present with 'python_version'\n",
      "Do this:\n",
      "open Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\n",
      "Type pipenv lock to create the Pipfile.lock.\n",
      "Done. Continue what you were doing\n",
      "Your Pipfile.lock (221d14) is out of date (during Docker build) If during running the  docker build command, you get an error like this:\n",
      "Your Pipfile.lock (221d14) is out of date. Expected: (939fe0).\n",
      "Usage: pipenv install [OPTIONS] [PACKAGES]...\n",
      "ERROR:: Aborting deploy\n",
      "Option 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\n",
      "Option 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\n",
      "pipenv  --rm\n",
      "rm Pipfile*\n",
      "You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run. Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\n",
      "Added by 🅱🅻🅰🆀\n",
      "Completed creating the environment locally but could not find the environment on AWS. Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\n",
      "Added by Edidiong Esu\n",
      "Installing waitress on Windows via GitBash: “waitress-serve” command not found Running 'pip install waitress' as a command on GitBash was not downloading the executable file 'waitress-serve.exe'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\n",
      "open a Jupyter notebook and run the same command ' pip install waitress'. This way the executable file will be downloaded. The notebook may give you this warning : 'WARNING: The script waitress-serve.exe is installed in 'c:\\Users\\....\\anaconda3\\Scripts' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.'\n",
      "Add the path where 'waitress-serve.exe' is installed into gitbash's PATH as such:\n",
      "enter the following command in gitbash: nano ~/.bashrc\n",
      "add the path to 'waitress-serve.exe' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\n",
      "close gitbash and open it again and you should be good to go\n",
      "Added by Bachar Kabalan\n",
      "Warning: the environment variable LANG is not set! Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\n",
      "This is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\n",
      "https://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\n",
      "But one can proceed without addressing it.\n",
      "Added by Abhirup Ghosh\n",
      "Module5 HW Question 6 The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\n",
      "Added by Quinn Avila\n",
      "Terminal Used in Week 5 videos: https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\n",
      "Added by Dawuta Smit\n",
      "waitress-serve shows Malformed application Question:\n",
      "When running\n",
      "pipenv run waitress-serve --listen=localhost:9696 q4-predict:app\n",
      "I get the following:\n",
      "There was an exception (ValueError) importing your module.\n",
      "It had these arguments:\n",
      "1. Malformed application 'q4-predict:app'\n",
      "Answer:\n",
      "Waitress doesn’t accept a dash in the python file name.\n",
      "The solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\n",
      "Added by Alex Litvinov\n",
      "Testing HTTP POST requests from command line using curl I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \n",
      "(Used with WSL2 on Windows, should also work on Linux and MacOS)\n",
      "curl --json '<json data>' <url>\n",
      "# piping the structure to the command\n",
      "cat <json file path> | curl --json @- <url>\n",
      "echo '<json data>' | curl --json @- <url>\n",
      "# example using piping\n",
      "echo '{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}'\\\n",
      "| curl --json @- http://localhost:9696/predict\n",
      "Added by Sylvia Schmitt\n",
      "NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms. Question:\n",
      "When executing\n",
      "eb local run  --port 9696\n",
      "I get the following error:\n",
      "ERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "Answer:\n",
      "There are two options to fix this:\n",
      "Re-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\n",
      "Edit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\n",
      "The disadvantage of the second approach is that the option might not be available the following years\n",
      "Added by Alex Litvinov\n",
      "Requests Error: No connection adapters were found for 'localhost:9696/predict'. You need to include the protocol scheme: 'http://localhost:9696/predict'.\n",
      "Without the http:// part, requests has no idea how to connect to the remote server.\n",
      "Note that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\n",
      "Added by George Chizhmak\n",
      "Getting the same result While running the docker image if you get the same result check which model you are using.\n",
      "Remember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\n",
      "Added by Ahmed Okka\n",
      "Trying to run a docker image I built but it says it’s unable to start the container process Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal\n",
      "How do I copy files from my local machine to docker container? You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "Hrithik Kumar Advani\n",
      "How do I copy files from a different folder into docker container’s working directory? You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\n",
      "COPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan\n",
      "I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video I struggled with the command :\n",
      "eb init -p docker tumor-diagnosis-serving -r eu-west-1\n",
      "Which resulted in an error when running : eb local run --port 9696\n",
      "ERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "I replaced it with :\n",
      "eb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\n",
      "This allowed the recognition of the Dockerfile and the build/run of the docker container.\n",
      "Added by Mélanie Fouesnard\n",
      "Dockerfile missing when creating the AWS ElasticBean environment I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\n",
      "ERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\n",
      "I did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\n",
      "Added by Mélanie Fouesnard\n",
      "How to get started with Week 6? Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "HW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\n",
      "FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n",
      "~~~Nukta Bhatia~~~\n",
      "How to get the training and validation metrics from XGBoost? During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\n",
      "We can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\n",
      "Added by Daniel Coronel\n",
      "How to solve regression problems with random forest in scikit-learn? You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\n",
      "Alena Kniazeva\n",
      "ValueError: feature_names must be string, and may not contain [, ] or < In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\n",
      "Solution description\n",
      "The cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\n",
      "features= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\n",
      "Asia Saeed\n",
      "Alternative Solution:\n",
      "In my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\n",
      "For me this works:\n",
      "features = []\n",
      "for f in dv.feature_names_:\n",
      "string = f.replace(“=<”, “-le”)\n",
      "features.append(string)\n",
      "Peter Ernicke\n",
      "`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model. If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\n",
      "Ali Osman\n",
      "Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=) If you’re getting TypeError:\n",
      "“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\n",
      "probably you’ve done this:\n",
      "features = dv.get_feature_names_out()\n",
      "It gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\n",
      "If you’re getting ValueError:\n",
      "“ValueError: feature_names must be string, and may not contain [, ] or <”,\n",
      "probably you’ve either done:\n",
      "features = list(dv.get_feature_names_out())\n",
      "or:\n",
      "features = dv.feature_names_\n",
      "reason is what you get from DictVectorizer here looks like this:\n",
      "['households',\n",
      "'housing_median_age',\n",
      "'latitude',\n",
      "'longitude',\n",
      "'median_income',\n",
      "'ocean_proximity=<1H OCEAN',\n",
      "'ocean_proximity=INLAND',\n",
      "'population',\n",
      "'total_bedrooms',\n",
      "'total_rooms']\n",
      "it has symbols XGBoost doesn’t like ([, ] or <).\n",
      "What you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\n",
      "import re\n",
      "features = dv.feature_names_\n",
      "pattern = r'[\\[\\]<>]'\n",
      "features = [re.sub(pattern, '  ', f) for f in features]\n",
      "Added by Andrii Larkin\n",
      "How to Install Xgboost To install Xgboost, use the code below directly in your jupyter notebook:\n",
      "(Pip 21.3+ is required)\n",
      "pip install xgboost\n",
      "You can update your pip by using the code below:\n",
      "pip install --upgrade pip\n",
      "For more about xgbboost and installation, check here:\n",
      "https://xgboost.readthedocs.io/en/stable/install.html\n",
      "Aminat Abolade\n",
      "What is eta in XGBoost Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\n",
      "ETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.\n",
      "What is the difference between bagging and boosting? For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\n",
      "Random Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\n",
      "XGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\n",
      "Note that boosting is not necessarily better than bagging.\n",
      "Mélanie Fouesnard\n",
      "Bagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\n",
      "Boosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\n",
      "Rileen\n",
      "Capture stdout for each iterations of a loop separately I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\n",
      "Using the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\n",
      "# This would be the content of the Jupyter Notebook cell\n",
      "from IPython.utils.capture import capture_output\n",
      "import sys\n",
      "different_outputs = {}\n",
      "for i in range(3):\n",
      "with capture_output(sys.stdout) as output:\n",
      "print(i)\n",
      "print(\"testing capture\")\n",
      "different_outputs[i] = output.stdout\n",
      "# different_outputs\n",
      "# {0: '0\\ntesting capture\\n',\n",
      "#  1: '1\\ntesting capture\\n',\n",
      "#  2: '2\\ntesting capture\\n'}\n",
      "Added by Sylvia Schmitt\n",
      "ValueError: continuous format is not supported Calling roc_auc_score() to get auc is throwing the above error.\n",
      "Solution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\n",
      "roc_auc_score(y_train, y_pred)\n",
      "Hareesh Tummala\n",
      "Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value? When rmse stops improving means, when it stops to decrease or remains almost similar.\n",
      "Pastor Soto\n",
      "One of the method to visualize the decision trees dot_data = tree.export_graphviz(regr, out_file=None,\n",
      "feature_names=boston.feature_names,\n",
      "filled=True)\n",
      "graphviz.Source(dot_data, format=\"png\")\n",
      "Krishna Anand\n",
      "from sklearn import tree\n",
      "tree.plot_tree(dt,feature_names=dv.feature_names_)\n",
      "Added By Ryan Pramana\n",
      "ValueError: Unknown label type: 'continuous' Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\n",
      "Alejandro Aponte\n",
      "Different values of auc, each time code is re-run When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\n",
      "Solution: try setting the random seed e.g\n",
      "dt = DecisionTreeClassifier(random_state=22)\n",
      "Bhaskar Sarma\n",
      "Does it matter if we let the Python file create the server or if we run gunicorn directly? They both do the same, it's just less typing from the script.\n",
      "Asked by Andrew Katoch, Added by Edidiong Esu\n",
      "No module named ‘ping’? When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\n",
      "\n",
      "from [file name] import ping\n",
      "Olga Rudakova\n",
      "DictVectorizer feature names The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\n",
      "Quinn Avila\n",
      "Does it matter if we let the Python file create the server or if we run gunicorn directly? They both do the same, it's just less typing from the script.\n",
      "ValueError: feature_names must be string, and may not contain [, ] or < This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\n",
      "You can address this error by replacing problematic characters in the feature names with underscores, like so:\n",
      "features = [f.replace('=<', '_').replace('=', '_') for f in features]\n",
      "This code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.\n",
      "Visualize Feature Importance by using horizontal bar chart To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\n",
      "1. # extract the feature importances from the model\n",
      "feature_importances = list(zip(features_names, rdr_model.feature_importances_))\n",
      "importance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\n",
      "2. # sort descending the dataframe by using feature_importances value\n",
      "importance_df = importance_df.sort_values(by='feature_importances', ascending=False)\n",
      "3. # create a horizontal bar chart\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\n",
      "plt.xlabel('Feature Importance')\n",
      "plt.ylabel('Feature Names')\n",
      "plt.title('Feature Importance Chart')\n",
      "Radikal Lukafiardi\n",
      "RMSE using metrics.root_meas_square() Instead of using np.sqrt() as the second step. You can extract it using like this way :\n",
      "mean_squared_error(y_val, y_predict_val,squared=False)\n",
      "Ahmed Okka\n",
      "Features Importance graph I like this visual implementation of features importance in scikit-learn library:\n",
      "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
      "It actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\n",
      "Ivan Brigida\n",
      "xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks. Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\n",
      "George Chizhmak\n",
      "Information Gain Information gain  in Y due to X, or the mutual information of Y and X\n",
      "Where  is the entropy of Y. \n",
      "\n",
      "If X is completely uninformative about Y:\n",
      "If X is completely informative about Y: )\n",
      "Hrithik Kumar Advani\n",
      "Data Leakage Filling in missing values using an entire dataset before splitting for training/testing/validation causes\n",
      "Serialized Model Xgboost error Save model by calling ‘booster.save_model’, see eg\n",
      "Load model:\n",
      "Dawuta Smit\n",
      "This section is moved to Projects\n",
      "How to get started with Week 8? TODO\n",
      "How to use Kaggle for Deep Learning? Create or import your notebook into Kaggle.\n",
      "Click on the Three dots at the top right hand side\n",
      "Click on Accelerator\n",
      "Choose T4 GPU\n",
      "Khurram Majeed\n",
      "How to use Google Colab for Deep Learning? Create or import your notebook into Google Colab.\n",
      "Click on the Drop Down at the top right hand side\n",
      "Click on “Change runtime type”\n",
      "Choose T4 GPU\n",
      "Khurram Majeed\n",
      "How do I push from Saturn Cloud to Github? Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\n",
      "Solution description: Follow the instructions in these github docs to create an SSH private and public key:\n",
      "https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\n",
      "y-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\n",
      "Then the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\n",
      "Or alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\n",
      "Click on your username and on manage\n",
      "Down below you will see the Git SSH keys section.\n",
      "Copy the default public key provided by Saturn Cloud\n",
      "Paste these key into the SSH keys section of your github repo\n",
      "Open a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\n",
      "You will receive a successful authentication notice.\n",
      "Odimegwu David\n",
      "Where is the Python TensorFlow template on Saturn Cloud? This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\n",
      "but the location shown in the video is no longer correct.\n",
      "This template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\n",
      "Steven Christolis\n",
      "Getting error module scipy not found during model training in Saturn Cloud tensorflow image The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the “Extra Packages” section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages.\n",
      "Sumeet Lalla\n",
      "How to upload kaggle data to Saturn Cloud? Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\n",
      "You can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\n",
      "On your notebook run:\n",
      "!pip install -q kaggle\n",
      "Go to Kaggle website (you need to have an account for this):\n",
      "Click on your profile image -> Account\n",
      "Scroll down to the API box\n",
      "Click on Create New API token\n",
      "It will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\n",
      "On the notebook click on folder icon on the left upper corner\n",
      "This will take you to the root folder\n",
      "Click on the .kaggle folder\n",
      "Once inside of the .kaggle folder upload the kaggle.json file that you downloaded\n",
      "Run this command on your notebook:\n",
      "!chmod 600 /home/jovyan/.kaggle/kaggle.json\n",
      "Download the data using this command:\n",
      "!kaggle datasets download -d agrigorev/dino-or-dragon\n",
      "Create a folder to unzip your files:\n",
      "!mkdir data\n",
      "Unzip your files inside that folder\n",
      "!unzip dino-or-dragon.zip -d data\n",
      "Pastor Soto\n",
      "How to install CUDA & cuDNN on Ubuntu 22.04 In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\n",
      "The process can be overwhelming. Here’s a simplified guide\n",
      "Osman Ali\n",
      "Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model. Problem description:\n",
      "When loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\n",
      "Solution description:\n",
      "Before loading model need to evaluate the model on input data: model.evaluate(train_ds)\n",
      "Added by Vladimir Yesipov\n",
      "Getting error when connect git on Saturn Cloud: permission denied Problem description:\n",
      "When follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\n",
      "Solution description:\n",
      "Alternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\n",
      "Added by Ryan Pramana\n",
      "Host key verification failed. Problem description:\n",
      "Getting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>\n",
      "The error:\n",
      "Cloning into 'clothing-dataset'...\n",
      "Host key verification failed.\n",
      "fatal: Could not read from remote repository.\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n",
      "Solution description:\n",
      "when cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.\n",
      "<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>\n",
      "Added by Gregory Morris\n",
      "The same accuracy on epochs Problem description\n",
      "The accuracy and the loss are both still the same or nearly the same while training.\n",
      "Solution description\n",
      "In the homework, you should set class_mode='binary' while reading the data.\n",
      "Also, problem occurs when you choose the wrong optimizer, batch size, or learning rate\n",
      "Added by Ekaterina Kutovaia\n",
      "Model breaking after augmentation – high loss + bad accuracy Problem:\n",
      "When resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\n",
      "Solution:\n",
      "Check that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\n",
      "Added by Konrad Mühlberg\n",
      "Missing channel value error while reloading model: While doing:\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "model = tf.keras.models.load_model('model_saved.h5')\n",
      "If you get an error message like this:\n",
      "ValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\n",
      "Solution:\n",
      "Saving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\n",
      "# model architecture:\n",
      "inputs = keras.Input(shape=(input_size, input_size, 3))\n",
      "base = base_model(inputs, training=False)\n",
      "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
      "inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
      "drop = keras.layers.Dropout(droprate)(inner)\n",
      "outputs = keras.layers.Dense(10)(drop)\n",
      "model = keras.Model(inputs, outputs)\n",
      "(Memoona Tahira)\n",
      "How to unzip a folder with an image dataset and suppress output? Problem:\n",
      "A dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\n",
      "Solution:\n",
      "Execute the next cell:\n",
      "%%capture\n",
      "! unzip zipped_folder_name.zip -d destination_folder_name\n",
      "Added by Alena Kniazeva\n",
      "Inside a Jupyter Notebook:\n",
      "import zipfile\n",
      "local_zip = 'data.zip'\n",
      "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
      "zip_ref.extractall('data')\n",
      "zip_ref.close()\n",
      "How keras flow_from_directory know the names of classes in images? Problem:\n",
      "When we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\n",
      "Solution:\n",
      "The name of class is the folder name\n",
      "If you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\n",
      "a clear explanation below:\n",
      "https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n",
      "Added by Bhaskar Sarma\n",
      "Error with scipy missing module in SaturnCloud Problem:\n",
      "I created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\n",
      "Solution:\n",
      "Install the module in a new cell: !pip install scipy\n",
      "Restart the kernel and fit the model again\n",
      "Added by Erick Calderin\n",
      "How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model: The command to read folders in the dataset in the tensorflow source code is:\n",
      "for subdir in sorted(os.listdir(directory)):\n",
      "…\n",
      "Reference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\n",
      "This means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\n",
      "When a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\n",
      "prob(class(0)) = 1- prob(class(1))\n",
      "In case of using from_logits to get results, you will get two values for each of the labels.\n",
      "A prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\n",
      "(Added by Memoona Tahira)\n",
      "Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class? It's fine, some small changes are expected\n",
      "Alexey Grigorev\n",
      "What if your accuracy and std training loss don’t match HW? Problem:\n",
      "I found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\n",
      "Solution:\n",
      "Try running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\n",
      "Added by Quinn Avila\n",
      "Using multi-threading for data generation in “model.fit()” When running “model.fit(...)” an additional parameter “workers” can be specified for speeding up the data loading/generation. The default value is “1”. Try out which value between 1 and the cpu count on your system performs best.\n",
      "https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
      "Added by Sylvia Schmitt\n",
      "Reproducibility with TensorFlow using a seed point Reproducibility for training runs can be achieved following these instructions: \n",
      "https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\n",
      "seed = 1234\n",
      "tf.keras.utils.set_random_seed(seed)\n",
      "tf.config.experimental.enable_op_determinism()\n",
      "This will work for a script, if this gets executed multiple times.\n",
      "Added by Sylvia Schmitt\n",
      "Can we use pytorch for this lesson/homework ? Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\n",
      "https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\n",
      "The functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\n",
      "Mélanie Fouesnard\n",
      "Keras model training fails with “Failed to find data adapter” While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\n",
      "train_gen = ImageDataGenerator(rescale=1./255)\n",
      "train_ds = train_gen.flow_from_directory(…)\n",
      "history_after_augmentation = model.fit(\n",
      "train_gen, # this should be train_ds!!!\n",
      "epochs=10,\n",
      "validation_data=test_gen # this should be test_ds!!!\n",
      ")\n",
      "The fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\n",
      "Added by Tzvi Friedman\n",
      "Running ‘nvidia-smi’ in a loop without using ‘watch’ The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\n",
      "nvidia-smi -l <N seconds>\n",
      "The following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\n",
      "nvidia-smi -l 2\n",
      "Added by Sylvia Schmitt\n",
      "Checking GPU and CPU utilization using ‘nvitop’ The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\n",
      "https://pypi.org/project//\n",
      "Image source: https://pypi.org/project//\n",
      "Added by Sylvia Schmitt\n",
      "Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from? Let’s say we define our Conv2d layer like this:\n",
      ">> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\n",
      "It means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\n",
      "If we check model.summary() we will get this:\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "conv2d (Conv2D)             (None, 148, 148, 32)      896\n",
      "So where does 896 params come from? It’s computed like this:\n",
      ">>> (3*3*3 +1) * 32\n",
      "896\n",
      "# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\n",
      "What about the number of “features” we get after the Flatten layer?\n",
      "For our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "max_pooling2d_3       (None, 7, 7, 128)         0\n",
      "flatten (Flatten)           (None, 6272)              0\n",
      "So where do 6272 vectors come from? It’s computed like this:\n",
      ">>> 7*7*128\n",
      "6272\n",
      "# 7x7 “image shape” after several convolutions and poolings, 128 filters\n",
      "Added by Andrii Larkin\n",
      "Sequential vs. Functional Model Modes in Keras (TF2) It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\n",
      "You can simply start from an “empty” model and add more and more layers in a sequential order.\n",
      "This mode is called “Sequential Model API”  (easier)\n",
      "In Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\n",
      "Maybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\n",
      "You can read more about it in this TF2 tutorial.\n",
      "A really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\n",
      "Added by Ivan Brigida\n",
      "Fresh Run on Neural Nets\n",
      "While correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\n",
      "Added by Abhijit Chakraborty\n",
      "Out of memory errors when running tensorflow I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\n",
      "https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n",
      "```\n",
      "physical_devices = tf.configlist_physical_devices('GPU')\n",
      "try:\n",
      "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
      "except:\n",
      "# Invalid device or cannot modify virtual devices once initialized.\n",
      "pass\n",
      "```\n",
      "Model training very slow in google colab with T4 GPU When training the models, in the fit function, you can specify the number of workers/threads.\n",
      "The number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\n",
      "I changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\n",
      "Added by Ibai Irastorza\n",
      "Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images From the keras documentation:\n",
      "Deprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\n",
      "Hrithik Kumar Advani\n",
      "How to get started with Week 9? TODO\n",
      "Where is the model for week 9? The week 9 uses a link to github to fetch the models.\n",
      "The original link was moved to here:\n",
      "https://github.com/DataTalksClub/machine-learning-zoomcamp/releases\n",
      "Executing the command echo ${REMOTE_URI} returns nothing. Solution description\n",
      "In the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\n",
      "I also had the same problem on Ubuntu terminal. I executed the following two commands:\n",
      "$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n",
      "$ echo $REMOTE_URI\n",
      "111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n",
      "Note: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\n",
      "2. Replace REMOTE_URI with your URI\n",
      "(Bhaskar Sarma)\n",
      "Getting a syntax error while trying to get the password from aws-cli The command aws ecr get-login --no-include-email returns an invalid choice error:\n",
      "The solution is to use the following command instead:  aws ecr get-login-password\n",
      "Could simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\n",
      "export PASSWORD=`aws ecr get-login-password`\n",
      "docker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\n",
      "Added by Martin Uribe\n",
      "Pass many parameters in the model at once We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\n",
      "Krishna Anand\n",
      "Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8 This error is produced sometimes when building your docker image from the Amazon python base image.\n",
      "Solution description: The following could solve the problem.\n",
      "Update your docker desktop if you haven’t done so.\n",
      "Or restart docker desktop and terminal and then build the image all over again.\n",
      "Or if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\n",
      "(optional) Added by Odimegwu David\n",
      "Problem: 'ls' is not recognized as an internal or external command, operable program or batch file. When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\n",
      "Solution description :\n",
      "Instead of !ls -lh , you can use this command !dir , and you will get similar output\n",
      "Asia Saeed\n",
      "ImportError: generic_type: type \"InterpreterWrapper\" is already registered! When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\n",
      "Solution description\n",
      "This error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\n",
      "Asia Saeed\n",
      "Windows version might not be up-to-date Problem description:\n",
      "In command line try to do $ docker build -t dino_dragon\n",
      "got this Using default tag: latest\n",
      "[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\n",
      "error during connect: This error may indicate that the docker daemon is not running.: Post\n",
      ".\n",
      "Solution description:\n",
      "You need to make sure that Docker is not stopped by a third-party program.\n",
      "Andrei Ilin\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available When running docker build -t dino-dragon-model it returns the above error\n",
      "The most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\n",
      "https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\n",
      "Pastor Soto\n",
      "How to do AWS configure after installing awscli Problem description:\n",
      "In video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\n",
      "Solution description:\n",
      "Yes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\n",
      "Added by Bhaskar Sarma\n",
      "Object of type float32 is not JSON serializable Problem:\n",
      "While passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\n",
      "{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\n",
      "This happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\n",
      "Solution:\n",
      "In my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\n",
      "preds = [interpreter.get_tensor(output_index)[0][0], \\\n",
      "1-interpreter.get_tensor(output_index)[0][0]]\n",
      "In which case the above described solution will look like this:\n",
      "preds = [float(interpreter.get_tensor(output_index)[0][0]), \\\n",
      "float(1-interpreter.get_tensor(output_index)[0][0])]\n",
      "The rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\n",
      "Added by Konrad Muehlberg\n",
      "Error with the line “interpreter.set_tensor(input_index, X”) I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\n",
      "ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\n",
      "This is because the X is an int but a float is expected.\n",
      "Solution:\n",
      "I found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\n",
      "# Need to convert to float32 before set_tensor\n",
      "X = np.float32(X)\n",
      "Then, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\n",
      "Added by Mélanie Fouesnard\n",
      "How to easily get file size in powershell terminal ? To check your file size using the powershell terminal, you can do the following command lines:\n",
      "$File = Get-Item -Path path_to_file\n",
      "$FileSize = (Get-Item -Path $FilePath).Length\n",
      "Now you can check the size of your file, for example in MB:\n",
      "Write-host \"MB\":($FileSize/1MB)\n",
      "Source: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\n",
      "Added by Mélanie Fouesnard\n",
      "How do Lambda container images work? I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\n",
      "https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\n",
      "https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\n",
      "Added by Alejandro aponte\n",
      "How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService? The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\n",
      "https://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\n",
      "Added by Sumeet Lalla\n",
      "Error building docker image on M1 Mac Problem:\n",
      "While trying to build docker image in Section 9.5 with the command:\n",
      "docker build -t clothing-model .\n",
      "It throws a pip install error for the tflite runtime whl\n",
      "ERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\n",
      "Try to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\n",
      "If the link above does not work:\n",
      "The problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\n",
      "Or try the code bellow.\n",
      "Added by Dashel Ruiz Perez\n",
      "Solution:\n",
      "To build the Docker image, use the command:\n",
      "docker build --platform linux/amd64 -t clothing-model .\n",
      "To run the built image, use the command:\n",
      "docker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\n",
      "Added by Daniel Egbo\n",
      "Error invoking API Gateway deploy API locally Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\n",
      "With error message:\n",
      "{'message': 'Missing Authentication Token'}\n",
      "Solution:\n",
      "Need to get the deployed API URL for the specific path you are invoking. Example:\n",
      "https://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\n",
      "Added by Andrew Katoch\n",
      "Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none) Problem: When trying to install tflite_runtime with\n",
      "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\n",
      "one gets an error message above.\n",
      "Solution:\n",
      "fflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\n",
      "your combination must be missing here\n",
      "you can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\n",
      "and install the needed one using pip\n",
      "eg\n",
      "pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\n",
      "as it is done in the lectures code:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\n",
      "Alternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\n",
      "Added by Alena Kniazeva, modified by Alex Litvinov\n",
      "Docker run error docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\n",
      "You need to restart the docker services to get rid of the above error\n",
      "Krishna Anand\n",
      "Save Docker Image to local machine and view contents The docker image can be saved/exported to tar format in local machine using the below command:\n",
      "docker image save <image-name> -o <name-of-tar-file.tar>\n",
      "The individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\n",
      "Sumeet Lalla\n",
      "Jupyter notebook not seeing package On vscode running jupyter notebook. After I ‘pip install pillow’, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked.\n",
      "Quinn Avila\n",
      "Running out of space for AWS instance. Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune\n",
      "Using Tensorflow 2.15 for AWS deployment Using the 2.14 version with python 3.11 works fine.\n",
      "In case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\n",
      "Added by Abhijit Chakraborty\n",
      "Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…” see here\n",
      "What IAM permission policy is needed to complete Week 9: Serverless? Sign in to the AWS Console: Log in to the AWS Console.\n",
      "Navigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\n",
      "Create a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\n",
      "Select the service and actions:\n",
      "Click on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\n",
      "Review and create the policy:\n",
      "Click on \"Review policy.\"\n",
      "Provide a name and description for the policy.\n",
      "Click on \"Create policy.\"\n",
      "JSON policy:\n",
      "{\n",
      "\"Version\": \"2012-10-17\",\n",
      "\"Statement\": [\n",
      "{\n",
      "\"Sid\": \"VisualEditor0\",\n",
      "\"Effect\": \"Allow\",\n",
      "\"Action\": [\n",
      "\"ecr:CreateRepository\",\n",
      "\"ecr:GetAuthorizationToken\",\n",
      "\"ecr:BatchCheckLayerAvailability\",\n",
      "\"ecr:BatchGetImage\",\n",
      "\"ecr:InitiateLayerUpload\",\n",
      "\"ecr:UploadLayerPart\",\n",
      "\"ecr:CompleteLayerUpload\",\n",
      "\"ecr:PutImage\"\n",
      "],\n",
      "\"Resource\": \"*\"\n",
      "}\n",
      "]\n",
      "}\n",
      "Added by: Daniel Muñoz-Viveros\n",
      "ERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\n",
      "(WSL2 system)\n",
      "Solved: Delete the file ~/.docker/config.json\n",
      "Yishan Zhan\n",
      "Docker Temporary failure in name resolution Add the next lines to vim /etc/docker/daemon.json\n",
      "{\n",
      "\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\n",
      "}\n",
      "Then, restart docker:  sudo service docker restart\n",
      "Ibai Irastorza\n",
      "Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer` Solution: add compile = False to the load_model function\n",
      "keras.models.load_model('model_name.h5', compile=False)\n",
      "Nadia Paz\n",
      "How to test AWS Lambda + Docker locally? This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\n",
      "Basically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\n",
      "docker run -it --rm -p 9000:8080 name\n",
      "This command runs the image as a container and starts up an endpoint locally at:\n",
      "localhost:9000/2015-03-31/functions/function/invocations\n",
      "Post an event to the following endpoint using a curl command:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n",
      "Examples of curl testing:\n",
      "* windows testing:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\"url\\\": \\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\"}\"\n",
      "* unix testing:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n",
      "If during testing you encounter an error like this:\n",
      "# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\n",
      "just turn your response at lambda_handler() to string - str(result).\n",
      "Added by Andrii Larkin\n",
      "\"Unable to import module 'lambda_function': No module named 'tensorflow'\" when run python test.py Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\n",
      "Added by Ryan Pramana\n",
      "Install Docker (udocker) in Google Colab I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\n",
      "https://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\n",
      "%%shell\n",
      "pip install udocker\n",
      "udocker --allow-root install\n",
      "!udocker --allow-root run hello-world\n",
      "Added by Ivan Brigida\n",
      "Lambda API Gateway errors:\n",
      "`Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header.`\n",
      "`Missing Authentication Token`\n",
      "import boto3\n",
      "client = boto3.client('apigateway')\n",
      "response = client.test_invoke_method(\n",
      "restApiId='your_rest_api_id',\n",
      "resourceId='your_resource_id',\n",
      "httpMethod='POST',\n",
      "pathWithQueryString='/test/predict', #depend how you set up the api\n",
      "body='{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n",
      ")\n",
      "print(response['body'])\n",
      "Yishan Zhan\n",
      "Unable to run pip install tflite_runtime from github wheel links?\n",
      "To overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\n",
      "COPY <file-name> .\n",
      "RUN pip install <file-name>\n",
      "Abhijit Chakraborty\n",
      "How to get started with Week 10? TODO\n",
      "How to install Tensorflow in Ubuntu WSL2 Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\n",
      "I was able to get it working by using the following resources:\n",
      "CUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\n",
      "Install TensorFlow with pip\n",
      "Start Locally | PyTorch\n",
      "I included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\n",
      "Added by Martin Uribe\n",
      "Getting: Allocator ran out of memory errors? If you are running tensorflow on your own machine and you start getting the following errors:\n",
      "Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "Try adding this code in a cell at the beginning of your notebook:\n",
      "config = tf.compat.v1.ConfigProto()\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "After doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\n",
      "Added by Martin Uribe\n",
      "Problem with recent version of protobuf In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      "1. Downgrade the protobuf package to 3.20.x or lower.\n",
      "2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "This will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\n",
      "pipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\n",
      "keras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\n",
      "Added by Ángel de Vicente\n",
      "WSL Cannot Connect To Docker Daemon Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\n",
      "”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\n",
      "Solution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\n",
      "Just enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\n",
      "Odimegwu David\n",
      "HPA instance doesn’t run properly In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\n",
      ">>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n",
      "And the targets still appear as <unknown>\n",
      "Run >>kubectl edit deploy -n kube-system metrics-server\n",
      "And search for this line:\n",
      "args:\n",
      "- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n",
      "Add this line in the middle:  - --kubelet-insecure-tls\n",
      "So that it stays like this:\n",
      "args:\n",
      "- --kubelet-insecure-tls\n",
      "- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n",
      "Save and run again >>kubectl get hpa\n",
      "Added by Marilina Orihuela\n",
      "HPA instance doesn’t run properly (easier solution) In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\n",
      ">>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n",
      "And the targets still appear as <unknown>\n",
      "Run the following command:\n",
      "kubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\n",
      "Which uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\n",
      "Added by Giovanni Pecoraro\n",
      "Could not install packages due to an OSError: [WinError 5] Access is denied When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Asia\\\\anaconda3\\\\Lib\\\\site-packages\\\\google\\\\protobuf\\\\internal\\\\_api_implementation.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "Solution description :\n",
      "I was able to install the libraries using below command:\n",
      "pip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\n",
      "Asia Saeed\n",
      "TypeError: Descriptors cannot not be created directly. Problem description\n",
      "I was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\n",
      "File \"C:\\Users\\Asia\\Data_Science_Code\\Zoompcamp\\Kubernetes\\gat.py\", line 9, in <module>\n",
      "from tensorflow_serving.apis import predict_pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow_serving\\apis\\predict_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py\", line 36, in <module>\n",
      "_descriptor.FieldDescriptor(\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 560, in __new__\n",
      "_message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      "1. Downgrade the protobuf package to 3.20.x or lower.\n",
      "2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "Solution description:\n",
      "Issue has been resolved by downgrading protobuf to version 3.20.1.\n",
      "pipenv install protobuf==3.20.1\n",
      "Asia Saeed\n",
      "How to install easily kubectl on windows ? To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\n",
      "I first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\n",
      "At step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\n",
      "Then I added this folder path to PATH in my environment variables.\n",
      "Kind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\n",
      "Added by Mélanie Fouesnard\n",
      "Install kind through choco library First you need to launch a powershell terminal with administrator privilege.\n",
      "For this we need to install choco library first through the following syntax in powershell:\n",
      "Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n",
      "Krishna Anand\n",
      "Install Kind via Go package If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go.\n",
      "> Download and Install Go (https://go.dev/doc/install)\n",
      "> Confirm installation by typing the following in Command Prompt -  go version\n",
      "> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0\n",
      ">Confirm Installation kind --version\n",
      "It works perfectly.\n",
      "The connection to the server localhost:8080 was refused - did you specify the right host or port? I ran into an issue where kubectl wasn't working.\n",
      "I kept getting the following error:\n",
      "kubectl get service\n",
      "The connection to the server localhost:8080 was refused - did you specify the right host or port?\n",
      "I searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\n",
      "All hogwash.\n",
      "The solution to my problem was to just start over.\n",
      "kind delete cluster\n",
      "rm -rf ~/.kube\n",
      "kind create cluster\n",
      "Now when I try the same command again:\n",
      "kubectl get service\n",
      "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\n",
      "Added by Martin Uribe\n",
      "Running out of storage after building many docker images Problem description\n",
      "Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\n",
      "My first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\n",
      "Solution description\n",
      "> docker images\n",
      "revealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\n",
      "a bunch of those — but to no avail!\n",
      "It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\n",
      "> docker system prune\n",
      "See also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\n",
      "Added by Konrad Mühlberg\n",
      "In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary? Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\n",
      "Pastor Soto\n",
      "Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean? In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\n",
      "cpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\n",
      "cpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\n",
      "These values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\n",
      "Added by Andrii Larkin\n",
      "Kind cannot load docker image Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\n",
      "kind load docker-image zoomcamp-10-model:xception-v4-001\n",
      "ERROR: no nodes found for cluster \"kind\"\n",
      "Solution: Specify cluster name with -n\n",
      "kind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\n",
      "Andrew Katoch\n",
      "'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows) Problem: I download kind from the next command:\n",
      "curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\n",
      "When I try\n",
      "kind --version\n",
      "I get: 'kind' is not recognized as an internal or external command, operable program or batch file\n",
      "Solution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\n",
      "Alejandro Aponte\n",
      "Running kind on Linux with Rootless Docker or Rootless Podman Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\n",
      "Sylvia Schmitt\n",
      "Kubernetes-dashboard Deploy and Access the Kubernetes Dashboard\n",
      "Luke\n",
      "Correct AWS CLI version for eksctl Make sure you are on AWS CLI v2 (check with aws --version)\n",
      "https://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html\n",
      "TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask Problem Description:\n",
      "In video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\n",
      "Solution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\n",
      "By running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\n",
      "Added by Bhaskar Sarma\n",
      "Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…” As per AWS documentation:\n",
      "https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\n",
      "You need to do: (change the fields in red)\n",
      "aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\n",
      "Alternatively you can run the following command without changing anything given you have a default region configured\n",
      "aws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\n",
      "Added by Humberto Rodriguez\n",
      "Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac While trying to run the docker code on M1:\n",
      "docker run --platform linux/amd64 -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "tensorflow/serving:2.7.0\n",
      "It outputs the error:\n",
      "Error:\n",
      "Status: Downloaded newer image for tensorflow/serving:2.7.0\n",
      "[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\n",
      "terminate called after throwing an instance of 'google::protobuf::FatalException'\n",
      "what():  CHECK failed: file != nullptr:\n",
      "qemu: uncaught target signal 6 (Aborted) - core dumped\n",
      "/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
      "Solution\n",
      "docker pull emacski/tensorflow-serving:latest\n",
      "docker run -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "emacski/tensorflow-serving:latest-linux_arm64\n",
      "See more here: https://github.com/emacski/tensorflow-serving-arm\n",
      "Added by Daniel Egbo\n",
      "Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well) Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\n",
      "Problem:\n",
      "While trying to run the docker code on Mac M2 apple silicon:\n",
      "docker run --platform linux/amd64 -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "tensorflow/serving\n",
      "You get an error:\n",
      "/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
      "Solution:\n",
      "Use bitnami/tensorflow-serving base image\n",
      "Launch it either using docker run\n",
      "docker run -d \\\n",
      "--name tf_serving \\\n",
      "-p 8500:8500 \\\n",
      "-p 8501:8501 \\\n",
      "-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\n",
      "-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\n",
      "bitnami/tensorflow-serving:2\n",
      "Or the following docker-compose.yaml\n",
      "version: '3'\n",
      "services:\n",
      "tf_serving:\n",
      "image: bitnami/tensorflow-serving:2\n",
      "volumes:\n",
      "- ${PWD}/clothing-model:/bitnami/model-data/1\n",
      "ports:\n",
      "- 8500:8500\n",
      "- 8501:8501\n",
      "environment:\n",
      "- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\n",
      "And run it with\n",
      "docker compose up\n",
      "Added by Alex Litvinov\n",
      "HPA doesn’t show CPU metrics Problem: CPU metrics Shows Unknown\n",
      "NAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\n",
      "credit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\n",
      "FailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\n",
      "Solution:\n",
      "-> Delete HPA (kubectl delete hpa credit-hpa)\n",
      "-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\n",
      "-> Create HPA\n",
      "This should solve the cpu metrics report issue.\n",
      "Added by Priya V\n",
      "Errors with istio during installation Problem description:\n",
      "Running this:\n",
      "curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\n",
      "Fails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\n",
      "Check kubectl version with kubectl version\n",
      "Solution description\n",
      "Edit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\n",
      "Run the bash script now.\n",
      "Added by Andrew Katoch\n",
      "Problem title Problem description\n",
      "Solution description\n",
      "(optional) Added by Name\n",
      "What are the project deadlines? Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.\n",
      "Are projects solo or collaborative/group work? Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]\n",
      "What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz? Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\n",
      "Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\n",
      "More discussions:\n",
      "[source1] [source2] [source3]\n",
      "Crucial Links These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\n",
      "Midterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\n",
      "MidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\n",
      "Submit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\n",
      "Datasets:\n",
      "https://www.kaggle.com/datasets and https://www.kaggle.com/competitions\n",
      "https://archive.ics.uci.edu/ml/index.php\n",
      "https://data.europa.eu/en\n",
      "https://www.openml.org/search?type=data\n",
      "https://newzealand.ai/public-data-sets\n",
      "https://datasetsearch.research.google.com\n",
      "What to do and Deliverables\n",
      "Think of a problem that's interesting for you and find a dataset for that\n",
      "Describe this problem and explain how a model could be used\n",
      "Prepare the data and doing EDA, analyze important features\n",
      "Train multiple models, tune their performance and select the best model\n",
      "Export the notebook into a script\n",
      "Put your model into a web service and deploy it locally with Docker\n",
      "Bonus points for deploying the service to the cloud\n",
      "How to conduct peer reviews for projects? Answer: Previous cohorts projects page has instructions (youtube).\n",
      "https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\n",
      "Alexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\n",
      "~~~ Added by Nukta Bhatia ~~~\n",
      "Computing the hash for project review See the answer here.\n",
      "Learning in public links for the projects For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\n",
      "14 posts, one for each day\n",
      "My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution? You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\n",
      "Ryan Pramana\n",
      "What If I submitted only two projects and failed to submit the third? If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\n",
      "(optional) David Odimegwu\n",
      "I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right? Yes. You only need to review peers when you submit your project.\n",
      "Confirmed on Slack by Alexey Grigorev (added by Rileen Sinha)\n",
      "How many models should I train? Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you're on the right track.\n",
      "How does the project evaluation work for you as a peer reviewer? I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\n",
      "Answer:\n",
      "The link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\n",
      "To calculate your hash value run the python code below:\n",
      "from hashlib import sha1\n",
      "def compute_hash(email):\n",
      "return sha1(email.lower().encode('utf-8')).hexdigest()\n",
      "# Example usage **** enter your email below (Example1@gmail.com)****\n",
      "email = \"Example1@gmail.com\"\n",
      "hashed_email = compute_hash(email)\n",
      "print(\"Original Email:\", email)\n",
      "print(\"Hashed Email (SHA-1):\", hashed_email)\n",
      "Edit the above code to replace Example1@gmail.com as your email address\n",
      "Store and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\n",
      "You then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\n",
      "Lastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\n",
      "By Emmanuel Ayeni\n",
      "Do you pass a project based on the average of everyone else’s scores or based on the total score you earn? Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\n",
      "Other course-related questions that don’t fall into any of the categories above or can apply to more than one category/module\n",
      "Why do I need to provide a train.py file when I already have the notebook.ipynb file? Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\n",
      "Odimegwu David\n",
      "Loading the Image with PILLOW library and converting to numpy array Pip install pillow - install pillow library\n",
      "from PIL import Image\n",
      "img = Image.open('aeroplane.png')\n",
      "From numpy import asarray\n",
      "numdata=asarray(img)\n",
      "Krishna Anand\n",
      "Is a train.py file necessary when you have a train.ipynb file in your midterm project directory? Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\n",
      "Is there a way to serve up a form for users to enter data for the model to crunch on? Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\n",
      "You can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\n",
      "Alejandro Aponte\n",
      "How to get feature importance for XGboost model Using model.feature_importances_ can gives you an error:\n",
      "AttributeError: 'Booster' object has no attribute 'feature_importances_'\n",
      "Answer: if you train the model like this: model = xgb.train you should use get_score() instead\n",
      "Ekaterina Kutovaia\n",
      "[Errno 12] Cannot allocate memory in AWS Elastic Container Service In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\n",
      "Just increase the RAM and CPU in your task definition.\n",
      "Humberto Rodriguez\n",
      "Pickle error: can’t get attribute XXX on module __main__ When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\n",
      "This does not happen when Flask is used directly, i.e. not through waitress.\n",
      "The problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\n",
      "When using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\n",
      "Solution:\n",
      "Put the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\n",
      "Note: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\n",
      "Detailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\n",
      "Marcos MJD\n",
      "How to handle outliers in a dataset? There are different techniques, but the most common used are the next:\n",
      "Dataset transformation (for example, log transformation)\n",
      "Clipping high values\n",
      "Dropping these observations\n",
      "Alena Kniazeva\n",
      "Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn' I was getting the below error message when I was trying to create docker image using bentoml\n",
      "[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn'\n",
      "Solution description\n",
      "The cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\n",
      "packages: # Additional pip packages required by the service\n",
      "- xgboost\n",
      "- scikit-learn\n",
      "- pydantic\n",
      "Asia Saeed\n",
      "BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\n",
      "Potential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\n",
      "(Memoona Tahira)\n",
      "Reproducibility Problem description:\n",
      "Do we have to run everything?\n",
      "You are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\n",
      "Not everyone will be able to run all the files, in particular the neural networks.\n",
      "Solution description:\n",
      "Alternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\n",
      "Related slack conversation here.\n",
      "(Gregory Morris)\n",
      "Model too big If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\n",
      "Quinn Avila\n",
      "Permissions to push docker to Google Container Registry When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\n",
      "gcloud auth configure-docker\n",
      "(Jesus Acuña)\n",
      "Tflite_runtime unable to install I am getting this error message when I tried to install tflite in a pipenv environment\n",
      "Error:  An error occurred while installing tflite_runtime!\n",
      "Error text:\n",
      "ERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\n",
      "ERROR: No matching distribution found for tflite_runtime\n",
      "This version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\n",
      "Pastor Soto\n",
      "Check all available versions here:\n",
      "https://google-coral.github.io/py-repo/tflite-runtime/\n",
      "If you don’t find a combination matching your setup, try out the options at\n",
      "https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\n",
      "which you can install as shown in the lecture, e.g.\n",
      "pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\n",
      "Finally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\n",
      "Rileen Sinha (based on discussions on Slack)\n",
      "Error when running ImageDataGenerator.flow_from_dataframe Error: ImageDataGenerator name 'scipy' is not defined.\n",
      "Check that scipy is installed in your environment.\n",
      "Restart jupyter kernel and try again.\n",
      "Marcos MJD\n",
      "How to pass BentoML content / docker container to Amazon Lambda Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\n",
      "https://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\n",
      "Konrad Muehlberg\n",
      "Error UnidentifiedImageError: cannot identify image file In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\n",
      "url = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\n",
      "X = preprocessor.from_url(url)\n",
      "I got the error:\n",
      "UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\n",
      "Solution:\n",
      "Add ?raw=true after .jpg in url. E.g. as below\n",
      "url = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\n",
      "Bhaskar Sarma\n",
      "[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\n",
      "Solution: Run: ` pipenv lock` for fix this problem and dependency files\n",
      "Alejandro Aponte\n",
      "Get_feature_names() not found Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\n",
      "Old: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\n",
      "New: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\n",
      "Solution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\n",
      "Ibai Irastorza\n",
      "Error decoding JSON response: Expecting value: line 1 column 1 (char 0) Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\n",
      "The problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\n",
      "Ahmed Okka\n",
      "Free cloud alternatives Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\n",
      "I think .5GB RAM is not enough, is there any other free alternative available ?\n",
      "A: aws (amazon), gcp (google), saturn.\n",
      "Both aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\n",
      "Saturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\n",
      "“You can sign up here: https://bit.ly/saturn-mlzoomcamp\n",
      "When you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\n",
      "Added by Andrii Larkin\n",
      "Getting day of the year from day and month column Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\n",
      "Solution description:\n",
      "convert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\n",
      "convert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\n",
      "convert day and month into a datetime object with:\n",
      "df['date_formatted'] = pd.to_datetime(\n",
      "dict(\n",
      "year='2055',\n",
      "month=df['month'],\n",
      "day=df['day']\n",
      ")\n",
      ")\n",
      "get day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\n",
      "(Bhaskar Sarma)\n",
      "Chart for classes and predictions How to visualize the predictions per classes after training a neural net\n",
      "Solution description\n",
      "classes, predictions = zip(*dict(zip(classes, predictions)).items())\n",
      "plt.figure(figsize=(12, 3))\n",
      "plt.bar(classes, predictions)\n",
      "Luke\n",
      "Convert dictionary values to Dataframe table You can convert the prediction output values to a datafarme using \n",
      "df = pd.DataFrame.from_dict(dict, orient='index' , columns=[\"Prediction\"])\n",
      "Edidiong Esu\n",
      "Kitchenware Classification Competition Dataset Generator The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\n",
      "It can be found here: kitchenware-dataset-generator | Kaggle\n",
      "Martin Uribe\n",
      "CUDA toolkit and cuDNN Install for Tensorflow Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\n",
      "Windows:\n",
      "Install Anaconda prompt https://www.anaconda.com/\n",
      "Two options:\n",
      "Install package ‘tensorflow-gpu’ in Anaconda\n",
      "Install the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\n",
      "WSL/Linux:\n",
      "WSL: Use the Windows Nvida drivers, do not touch that.\n",
      "Two options:\n",
      "Install the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\n",
      "Make sure to follow step 4 to install CUDA by environment\n",
      "Also run:\n",
      "echo ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
      "Install CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\n",
      "Install https://developer.nvidia.com/rdp/cudnn-download\n",
      "Now you should be able to do training/inference with GPU in Tensorflow\n",
      "(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\n",
      "ANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\n",
      "ezehcp7482@gmail.com:\n",
      "PROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.\n",
      "ANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)\n",
      "About getting the wrong result when multiplying matrices When multiplying matrices, the order of multiplication is important.\n",
      "For example:\n",
      "A (m x n) * B (n x p) = C (m x p)\n",
      "B (n x p) * A (m x n) = D (n x n)\n",
      "C and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\n",
      "Baran Akın\n",
      "None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip? Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\n",
      "(added by Rileen Sinha)\n",
      "I may end up submitting the assignment late. Would it be evaluated? Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\n",
      "(Added by Rileen Sinha, based on answer by Alexey on Slack)\n",
      "Does the github repository need to be public? Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\n",
      "(added by Tano Bugelli)\n",
      "How to install Conda environment in my local machine?\n",
      "Which ide is recommended for machine learning?\n",
      "How to use wget with Google Colab? Install w get:\n",
      "!which wget\n",
      "Download data:\n",
      "!wget -P /content/drive/My\\ Drive/Downloads/ URL\n",
      "(added by Paulina Hernandez)\n",
      "Features in scikit-learn? Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\n",
      "Use reshape to reshape a 1D array to a 2D.\n",
      "\t\t\t\t\t\t\t(-Aileah) :>\n",
      "(added by Tano\n",
      "filtered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
      "# Select only the desired columns\n",
      "selected_columns = [\n",
      "'latitude',\n",
      "'longitude',\n",
      "'housing_median_age',\n",
      "'total_rooms',\n",
      "'total_bedrooms',\n",
      "'population',\n",
      "'households',\n",
      "'median_income',\n",
      "'median_house_value'\n",
      "]\n",
      "filtered_df = filtered_df[selected_columns]\n",
      "# Display the first few rows of the filtered DataFrame\n",
      "print(filtered_df.head())\n",
      "When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass? FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "Reproducibility in different OS When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\n",
      "```\n",
      "Warning: Python 3.11 was not found on your system…\n",
      "Neither ‘pipenv’ nor ‘asdf’ could be found to install Python.\n",
      "You can specify specific versions of Python with:\n",
      "$ pipenv –python path\\to\\python\n",
      "```\n",
      "The solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\n",
      "(Added by Abhijit Chakraborty)\n",
      "Deploying to Digital Ocean You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\n",
      "Steps:\n",
      "Register in DigitalOcean\n",
      "Go to Apps -> Create App.\n",
      "You will need to choose GitHub as a service provider.\n",
      "Edit Source Directory (if your project is not in the repo root)\n",
      "IMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\n",
      "Remember to add model files if they are not built automatically during the container build process.\n",
      "By Dmytro Durach\n",
      "Is it best to train your model only on the most important features? I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\n",
      "Not necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\n",
      "By Rileen Sinha\n",
      "How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows? You can consider several different approaches:\n",
      "Sampling: In the exploratory phase, you can use random samples of the data.\n",
      "Chunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\n",
      "Optimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\n",
      "Using Dask, an open-source python project which parallelizes Numpy and Pandas.\n",
      "(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\n",
      "By Rileen Sinha\n",
      "Can I do the course in other languages, like R or Scala? Technically, yes. Advisable? Not really. Reasons:\n",
      "Some homework(s) asks for specific python library versions.\n",
      "Answers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\n",
      "And as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\n",
      "You can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\n",
      "tx[source]\n",
      "Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"? Yes, it’s allowed (as per Alexey).\n",
      "Added By Rileen Sinha\n",
      "Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem? The TF and TF Serving versions have to match (as per solution from the slack channel)\n",
      "Added by Chiedu Elue\n",
      "Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile? I’ve seen LinkedIn users list DataTalksClub as Experience with titles as:\n",
      "Machine Learning Fellow\n",
      "Machine Learning Student\n",
      "Machine Learning Participant\n",
      "Machine Learning Trainee\n",
      "Please note it is best advised that you do not list the experience as an official “job” or “internship” experience since DataTalksClub did not hire you, nor financially compensate you.\n",
      "Other ways you can incorporate the experience in the following sections:\n",
      "Organizations\n",
      "Projects\n",
      "Skills\n",
      "Featured\n",
      "Original posts\n",
      "Certifications\n",
      "Courses\n",
      "By Annaliese Bronz\n",
      "Interesting question, I put the link of my project into my CV as showcase and make posts to show my progress.\n",
      "By Ani Mkrtumyan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "embedings=[]\n",
    "for doc in tqdm(document_ml_zoomcamp):\n",
    "\n",
    "    text=doc['text']\n",
    "    question=doc['question']\n",
    "    qa_text = f'{question} {text}'\n",
    "    print(qa_text)\n",
    "    qa_text_v=model.encode(qa_text)\n",
    "    embedings.append(qa_text_v)\n",
    "    doc[\"text_vector\"]=qa_text_v\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6506574"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(user_question_embeding).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchEngine():\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query, num_results=10):\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        idx = np.argsort(-scores)[:num_results]\n",
    "        return [self.documents[i] for i in idx]\n",
    "\n",
    "search_engine = VectorSearchEngine(documents=document_ml_zoomcamp, embeddings=X)\n",
    "def custom_search_engine_retrieval(query):\n",
    "    \n",
    "    query_embeding=np.array(model.encode(query['question']))\n",
    "    \n",
    "    return search_engine.search(query_embeding, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b5a931e38840f78f695eded7c9c1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=evaluate(ground_truth, custom_search_engine_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '08ac3c5a2cbf', 'cluster_name': 'docker-cluster', 'cluster_uuid': '3YWWp8nQRuiHOUGDjX1VDQ', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} ,\n",
    "            \"id\":{\"type\": \"text\"},\n",
    "            \"text_vector\": {\"type\": \"dense_vector\", \"dims\": 768, \"index\": True, \"similarity\": \"cosine\"},\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in document_ml_zoomcamp:\n",
    "    try:\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Create end user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "vector_search_term = model.encode(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"field\": \"text_vector\",\n",
    "    \"query_vector\": vector_search_term,\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 10000, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'course-questions',\n",
       "  '_id': 'rY0TyZABNrIKqxeCpdDc',\n",
       "  '_score': 0.82532895,\n",
       "  '_source': {'question': 'The course has already started. Can I still join it?',\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "   'id': 'ee58a693'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'sI0TyZABNrIKqxeCpdDy',\n",
       "  '_score': 0.7358538,\n",
       "  '_source': {'question': 'I just joined. What should I do next? How can I access course materials?',\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "   'id': '0a278fb2'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'pI0TyZABNrIKqxeCpdCt',\n",
       "  '_score': 0.72950006,\n",
       "  '_source': {'question': \"I filled the form, but haven't received a confirmation email. Is it normal?\",\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "   'id': '6ba259b1'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'Eo0TyZABNrIKqxeCq9JY',\n",
       "  '_score': 0.7284953,\n",
       "  '_source': {'question': 'Can I do the course in other languages, like R or Scala?',\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': 'Miscellaneous',\n",
       "   'text': 'Technically, yes. Advisable? Not really. Reasons:\\nSome homework(s) asks for specific python library versions.\\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\\nYou can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\\ntx[source]',\n",
       "   'id': '9f261648'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 's40TyZABNrIKqxeCpdD_',\n",
       "  '_score': 0.72527933,\n",
       "  '_source': {'question': 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?',\n",
       "   'course': 'machine-learning-zoomcamp',\n",
       "   'section': 'General course-related questions',\n",
       "   'text': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "   'id': 'e7ba6b8a'}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = es_client.search(index=index_name, knn=query, source=[\"text\", \"section\", \"question\", \"course\",\"id\"])\n",
    "res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_knn(query_vector):\n",
    "    knn = {\n",
    "    \"field\": \"text_vector\",\n",
    "    \"query_vector\": query_vector,\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 10000, \n",
    "}\n",
    "\n",
    "\n",
    "    es_results = es_client.search(index=index_name, knn=knn, source=[\"text\", \"section\", \"question\", \"course\",\"id\"])\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in es_results['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2869a1670d48eb8a02a40c25b67885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=evaluate(ground_truth, elastic_search_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.01366120218579235}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I made a minor modification to the query shown in the notebook here\n",
    "# (compare to the one shown in the video)\n",
    "# Included \"knn\" in the search query (to perform a semantic search) along with the filter  \n",
    "knn_query = {\n",
    "    \"field\": \"text_vector\",\n",
    "    \"query_vector\": vector_search_term,\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 10000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = es_client.search(\n",
    "    index=\"course-questions\",\n",
    "    query={\n",
    "        \"match\": {\"section\": \"General course-related questions\"},\n",
    "    },\n",
    "    knn=knn_query,\n",
    "    size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://logz.io/blog/elasticsearch-mapping/#:~:text=Within%20a%20search%20engine%2C%20mapping,indexes%20and%20stores%20its%20fields\n",
    "\n",
    "2. https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "\n",
    "3. https://www.elastic.co/search-labs/tutorials\n",
    "\n",
    "4. https://www.elastic.co/search-labs/blog/text-similarity-search-with-vectors-in-elasticsearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
